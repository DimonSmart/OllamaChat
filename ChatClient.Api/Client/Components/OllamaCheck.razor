@using ChatClient.Api.Services
@using ChatClient.Shared.Services
@using ChatClient.Shared.Models
@using MudBlazor
@inject IOllamaClientService OllamaService
@inject IOpenAIClientService OpenAIService
@inject IUserSettingsService UserSettingsService
@inject ILlmServerConfigService LlmServerConfigService
@inject NavigationManager NavigationManager
@inject ILogger<OllamaCheck> Logger

@if (isChecking)
{
    <MudStack Justify="Justify.Center" AlignItems="AlignItems.Center" Style="height: 50vh;">
        <div class="text-center">
            <MudProgressCircular Size="Size.Large" Indeterminate="true" />
            <MudText Typo="Typo.h6" Class="mt-4">Checking server status...</MudText>
        </div>
    </MudStack>
}
else
{
    @ChildContent
}

@code {
    [Parameter] public RenderFragment? ChildContent { get; set; }

    private bool isChecking = true;

    protected override async Task OnInitializedAsync()
    {
        await CheckServerStatus();
    }

    private async Task CheckServerStatus()
    {
        isChecking = true;
        try
        {
            var settings = await UserSettingsService.GetSettingsAsync();
            if (!ValidateDefaultServer(settings))
                return;

            var server = await GetDefaultServerAsync(settings);
            if (server == null)
                return;

            await ValidateServerAvailability(server, settings.DefaultLlmId!.Value);
        }
        catch (Exception ex)
        {
            Logger.LogWarning(ex, "Server not available for page access");
            NavigateToServerConfig();
        }
        finally
        {
            isChecking = false;
            StateHasChanged();
        }
    }

    private bool ValidateDefaultServer(UserSettings settings)
    {
        if (settings.DefaultLlmId.HasValue)
            return true;

        Logger.LogWarning("No default server configured");
        NavigateToServerConfig();
        return false;
    }

    private async Task<LlmServerConfig?> GetDefaultServerAsync(UserSettings settings)
    {
        var servers = await LlmServerConfigService.GetAllAsync();
        return LlmServerConfigHelper.GetServerConfig(servers, settings.DefaultLlmId);
    }

    private async Task ValidateServerAvailability(LlmServerConfig server, Guid serverId)
    {
        switch (server.ServerType)
        {
            case ServerType.Ollama:
                await ValidateOllamaServer(server, serverId);
                break;
            case ServerType.ChatGpt:
                await ValidateOpenAIServer(server, serverId);
                break;
            default:
                Logger.LogWarning("Unknown server type: {ServerType}", server.ServerType);
                NavigateToServerConfig();
                break;
        }
    }

    private async Task ValidateOllamaServer(LlmServerConfig server, Guid serverId)
    {
        await OllamaService.GetModelsAsync(serverId);
        Logger.LogInformation("Ollama server '{ServerName}' is available", server.Name);
    }

    private async Task ValidateOpenAIServer(LlmServerConfig server, Guid serverId)
    {
        var isAvailable = await OpenAIService.IsAvailableAsync(serverId);
        if (!isAvailable)
        {
            Logger.LogWarning("OpenAI server '{ServerName}' is not available", server.Name);
            NavigateToServerConfig();
            return;
        }
        Logger.LogInformation("OpenAI server '{ServerName}' is available", server.Name);
    }

    private void NavigateToServerConfig()
    {
        NavigationManager.NavigateTo("/llm-servers", true);
    }
}
