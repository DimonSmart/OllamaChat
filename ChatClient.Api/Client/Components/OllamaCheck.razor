@using ChatClient.Api.Services
@using ChatClient.Application.Services
@using ChatClient.Domain.Models
@using Microsoft.AspNetCore.Components
@using MudBlazor
@inject IOllamaClientService OllamaService
@inject IOpenAIClientService OpenAIService
@inject IUserSettingsService UserSettingsService
@inject ILlmServerConfigService LlmServerConfigService
@inject NavigationManager NavigationManager
@inject ILogger<OllamaCheck> Logger

@if (isChecking)
{
    <MudStack Justify="Justify.Center" AlignItems="AlignItems.Center" Style="height: 50vh;">
        <div class="text-center">
            <MudProgressCircular Size="Size.Large" Indeterminate="true" />
            <MudText Typo="Typo.h6" Class="mt-4">Checking server status...</MudText>
        </div>
    </MudStack>
}
else
{
    @ChildContent
}

@code {
    [Parameter] public RenderFragment? ChildContent { get; set; }

    private bool isChecking = true;

    protected override async Task OnInitializedAsync()
    {
        await CheckServerStatus();
    }

    private async Task CheckServerStatus()
    {
        isChecking = true;
        try
        {
            var settings = await UserSettingsService.GetSettingsAsync();
            var servers = (await LlmServerConfigService.GetAllAsync()).ToList();
            if (servers.Count == 0)
            {
                Logger.LogWarning("No LLM servers configured");
                NavigateToServerConfig();
                return;
            }

            var hasAvailableServer = await HasAnyAvailableServerAsync(settings, servers);
            if (!hasAvailableServer)
            {
                Logger.LogWarning("No available LLM servers found");
                NavigateToServerConfig();
                return;
            }
        }
        catch (NavigationException)
        {
            // Expected during prerender when navigation triggers redirect.
        }
        catch (Exception ex)
        {
            Logger.LogWarning("Server not available for page access: {Message}", ex.Message);
            NavigateToServerConfig();
        }
        finally
        {
            isChecking = false;
            StateHasChanged();
        }
    }

    private async Task<bool> HasAnyAvailableServerAsync(UserSettings settings, IReadOnlyCollection<LlmServerConfig> servers)
    {
        var orderedServers = OrderServersByDefault(settings, servers);
        foreach (var server in orderedServers)
        {
            if (await IsServerAvailableAsync(server))
                return true;
        }

        return false;
    }

    private static IEnumerable<LlmServerConfig> OrderServersByDefault(UserSettings settings, IReadOnlyCollection<LlmServerConfig> servers)
    {
        var defaultServer = LlmServerConfigHelper.GetServerConfig(servers, settings.DefaultModel.ServerId);
        if (defaultServer is not null)
            yield return defaultServer;

        foreach (var server in servers.Where(s => s.Id != settings.DefaultModel.ServerId))
            yield return server;
    }

    private async Task<bool> IsServerAvailableAsync(LlmServerConfig server)
    {
        if (server.Id is null)
            return false;

        switch (server.ServerType)
        {
            case ServerType.Ollama:
                return await ValidateOllamaServer(server, server.Id.Value);
            case ServerType.ChatGpt:
                return await ValidateOpenAIServer(server, server.Id.Value);
            default:
                Logger.LogWarning("Unknown server type: {ServerType}", server.ServerType);
                return false;
        }
    }

    private async Task<bool> ValidateOllamaServer(LlmServerConfig server, Guid serverId)
    {
        try
        {
            await OllamaService.GetModelsAsync(serverId);
            Logger.LogInformation("Ollama server '{ServerName}' is available", server.Name);
            return true;
        }
        catch (Exception ex)
        {
            Logger.LogWarning("Ollama server '{ServerName}' is not available: {Message}", server.Name, ex.Message);
            return false;
        }
    }

    private async Task<bool> ValidateOpenAIServer(LlmServerConfig server, Guid serverId)
    {
        var isAvailable = await OpenAIService.IsAvailableAsync(serverId);
        if (!isAvailable)
        {
            Logger.LogWarning("OpenAI server '{ServerName}' is not available", server.Name);
            return false;
        }

        Logger.LogInformation("OpenAI server '{ServerName}' is available", server.Name);
        return true;
    }

    private void NavigateToServerConfig()
    {
        NavigationManager.NavigateTo("/llm-servers", true);
    }
}
